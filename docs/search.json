[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Computational Methods Workshop!",
    "section": "",
    "text": "Source: reddit.com\n\n\n\nWorkshop overview\nWe are excited to announce a series of computational workshops at the Manship School of Mass Communication in Spring 2025, made possible through the collaborative efforts of the BEACON Lab led by Drs. Ryan Wang & Josh Jackson (Behavioral and Empirical Approaches for COmputational Nexus) and the DiCCAL Research Group led by Drs. Janice Lee & Royce Choi (Digital Cognition and Computational Analytics Lab). The BEACON Lab primarily focuses on advancing (computational) methodologies as well as introducing analytical skills. The DiCCAL Research Group emphasizes applying these techniques within the fields of communication and social science.\nAs part of our mission to foster computational innovation, these intro-level workshops will introduce a range of computational methods tailored to graduate students‚Äô interests. The sessions will provide both foundational training and support for idea and proposal development. Workshops will take place on Feb 14th, Feb 28th, and Mar 21st, with each session lasting around two hours. Sessions will include pre-assigned readings, hands-on practice during the workshop, and follow-up exercises afterward.\n\n\nSuggested reading\n\nWickham, H., √áetinkaya-Rundel, M. and Grolemund, G. (2023). R for Data Science (2e)\n\n\n\nSpring 2025 Workshop schedule\n\n\n\nDates\nTopics\n\n\n\n\nSection 1 (Feb 14th)\nIntroduction to R\n\n\nSection 2 (Feb 28th)\nData visualization\n\n\nSection 3 (Mar 21st)\nTBD",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "prepare.html",
    "href": "prepare.html",
    "title": "Preparation before workshop",
    "section": "",
    "text": "R is a programming language, or the engine, but its command-line interface (the base R GUI) is not user friendly. That‚Äôs why we‚Äôd also need RStudio ‚Äì an integrated development environment (IDE) for R, or the dashboard, which is designed to make working with R easier and more efficient. You are free to use any other IDE for R as well (i.e., VS Code, and etc.).\nYou‚Äôd need to install R and RStudio seperatedly and you can follow the video instruction here.\n\nTo download R, you can choose your preferred CRAN mirror(somewhere near your location), or you can choose the 0-cloud option.\nTo download RStudio, you go to Posit‚Äôs website and follow the instruction based on your operation system (Windows/macOS/Ubuntu) here.\nIf you prefer a cloud version, Posit‚Äôs Cloud could be an option although I‚Äôd recommend to use a local version.",
    "crumbs": [
      "Preparation"
    ]
  },
  {
    "objectID": "prepare.html#rstudio-orientation",
    "href": "prepare.html#rstudio-orientation",
    "title": "Preparation before workshop",
    "section": "RStudio Orientation",
    "text": "RStudio Orientation\nThere are mainly four panels once you open the IDE (you can modify the theme and the layout under File - Preference - Appearance/Pane layout).\n\nConsole\nEnvironment/History/‚Ä¶\nFiles/Plots/Viewer/‚Ä¶\nSources",
    "crumbs": [
      "Preparation"
    ]
  },
  {
    "objectID": "prepare.html#reproducible-research",
    "href": "prepare.html#reproducible-research",
    "title": "Preparation before workshop",
    "section": "Reproducible research",
    "text": "Reproducible research\nReproducible: Can someone else reproduce your entire analysis?\n\nAvailable data\nAvailable codes (including the random seed for machine learning)\n\nWe will be mainly using two types of file formats (other related formats such as Rproj and Rdata:\n\nR script (a text file contains the same commands that your would enter on the command line of R)\nRMarkdown\n\nText, code, and results (from your analysis)\nFormatted output: html, pdf (which requires tex, a typesetting system), etc.\nResource: cheatsheet, The Definitive Guide\n\n\nR markdown is a simple and easy to use plain text language used to combine your R code, results from your data analysis (including plots and tables) and written commentary into a single nicely formatted and reproducible document (like a report, publication, thesis chapter or a web page like this one).\n\n\nOther examples of markup languages include (compared with Word):\n\nHTML (HyperText Markup Language): website\nLaTex: Overleaf\nMarkdown (a ‚Äúlightweight‚Äù markup language)",
    "crumbs": [
      "Preparation"
    ]
  },
  {
    "objectID": "prepare.html#r-packages",
    "href": "prepare.html#r-packages",
    "title": "Preparation before workshop",
    "section": "R packages",
    "text": "R packages\nUntil Feb 11th, 2025, there are 22021 available packages on CRAN (The Comprehensive R Archive Network) package repository.\n\n# Install packages\n## install.packages(\"xxx\") - if on CRAN\n## devtools::install_github(\"houyunhuang/ggcor\") - if only on github\n\n\n# Loading packages\nlibrary(dplyr)\n\n\n# An elegant way to install and load packages by using a loop\n if (!require(dplyr)) {\n    install.packages(\"dplyr\")\n    library(dplyr)\n}\n\n# With multiple packages\nsomepackages &lt;- c(\"dplyr\", \"plyr\", \"magrittr\")\n\nfor (pkg in somepackages) {\n  if (!require(pkg, character.only = TRUE)) {\n    install.packages(pkg)\n    library(pkg,character.only = TRUE)\n  }\n}\n\n\n# Unload packages\ndetach(\"package:dplyr\", unload = TRUE)\n\n# Remove packages\n## remove.packages(\"dplyr\")\n\n\nThe chunk options in R code\nGlobal options: knitr::opts_chunk$set(echo = FALSE)\n\nIn this example, the knitr::opts_chunk$set(echo = FALSE) line in the setup chunk tells R Markdown to hide the R code within code chunks for the entire document, except when overridden within individual code chunks using {r} options. This is often used to create clean and readable reports or documents where you want to present the results of your R code without cluttering the document with the code itself.\n\nOther (individual) chunk options:\n\ninclude = FALSE prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.\necho = FALSE prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.\nmessage = FALSE prevents messages that are generated by code from appearing in the finished file.\nwarning = FALSE prevents warnings that are generated by code from appearing in the finished.\nfig.cap = \"...\" adds a caption to graphical results.\n\n\n\nSome useful packages\n\nData loading\n\nxlsx: read and write Micorsoft Excel files from R\nhaven: enable R to read and write data from SAS, SPSS, and Stata.\n\nData wrangling\n\ntidyverse: a collection of R packages designed for data science that share an underlying design philosophy, grammar, and data structures, for data import, tidying, and visualization listed here.\ndplyr: essential shortcuts for subsetting, summarizing, rearranging, and joining together data sets.\ntidyr: tools for changing the layout of your data sets. Use the gather and spread functions to convert your data into the tidy format, the layout R likes best.\nstringr: easy to learn tools for regular expressions and character strings.\nlubridate: tools that make working with dates and times easier.\nsna / network: a range of tools for social network analysis.\n\nData visualization\n\nggplot2: R‚Äôs famous package for making beautiful graphics. ggplot2 lets you use the grammar of graphics to build layered, customizable plots.\nother extension of ggplot2\nigraph: a package for network analysis and visualization\n\nData modeling/analysis\n\ntidymodels: a collection of packages for modeling and machine learning using tidyverse principles\ncaret: tools for classification and regression training\ncar: a hands-on companion to applied regression, especially the anova and vif function.\nlme4: linear and Non-linear mixed effects models\nquanteda: an R package for managing and analyzing text.\nstatnet: a suite of open source R-based software packages for (statistical) network analysis",
    "crumbs": [
      "Preparation"
    ]
  },
  {
    "objectID": "prepare.html#r-basic-operators",
    "href": "prepare.html#r-basic-operators",
    "title": "Preparation before workshop",
    "section": "R basic operators",
    "text": "R basic operators\n\nArithmetic operators\n\n1 + 19 # addition\n19 - 1 # subtraction\n5 * 4 # multiplication\n10 / 2 # division\n11 %/% 2 # integer division\n41 %% 21 # modulus\n20 ^ 2 # exponents\n20 ** 2\n\n\ndata &lt;- data.frame(x1 = 1:3,  \n                      x2 = 2:4,\n                      x3 = 2)\ndata \n\n\ndata^2\n\n\n\nThe &lt;- operator\nAssignment is a binary operator: the left side is a symbol/variable/object, the right is a value/expression being assigned to the left.\n\nx &lt;- 1\nx &lt;- c(1, 2, 3, 4, 5)\nx &lt;- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\n\n\n\nThe [] operator\nIndexing is a binary operator (two operands: the object being indexed (e.g., a vector, list, or data frame) and the index or indices used to select specific elements from that object. )\n\nx &lt;- c(5, 4, 3, 2, 1)\nx[1] # Extracts the first element\n\n\nx &lt;- data.frame(x1 = 1:3,  \n                x2 = 2:4,\n                x3 = 2)\nx[3] \nx[,3]\nx[3,]\nx[3,2]\nx[\"x3\"]\n\n\n\nThe $ operator\nThe $ operator is used to extract or subset a specific part of a data object in R.\n\nExtract the values in a data frame columns\n\n\ndata &lt;- data.frame(x1 = 1:5,  # Create example data\n                   x2 = letters[1:5],\n                   x3 = 9)\ndata  \n\n\ndata$x2\n\n\nReturn specific list elements\n\n\nmy_list &lt;- list(A = 1:5,  # Create example list\n                B = letters[1:5],\n                C = 9)\nmy_list # Print example list\n\n\nmy_list$B # Extract element of list\n\n\n\nThe () operator\nA function call is also a binary operator as the left side is a symbol pointing to the function argument and the right side are the arguments\n\nmax(1,2)\nx &lt;- max(1,2)\n\n\n\nThe ? operator\n\n?: Search R documentation for a specific term.\n??: Search R help files for a word or phrase.\n\n\n\nThe %&gt;% or |&gt; opertor\n%&gt;% is a longstanding feature of the magrittr package for R. It takes the output of one function and passes it into another function as an argument. This allows us to link a sequence of analysis steps (think about a a conveyor belt in a factory)\n\nReadability and clarity\nEast of modification\nAvoid intermediate variables\n\n\nlibrary(tidyverse)\n?group_by\n?mtcars\nmtcars\n\nx &lt;- filter(mtcars, cyl == 6)\ny &lt;- select(x, c(\"mpg\", \"hp\"))\n\nmtcars |&gt;\n  filter(cyl == 6) |&gt;\n  select(mpg, hp)\n\n\nresult &lt;- mtcars |&gt;\n  group_by(cyl) |&gt;\n  summarise(meanMPG = mean(mpg))\n\n\n\nThe %in% opertor\n%in% is a matching feature to check if the values of the first argument are present in the second argument and returns a logical vector indicating if there is a match or not for its left operand. Here, the first and second arguments can be a value, vector, list, or sequence.\n\n# Check value in a Vector\n67 %in% c(2,5,8,23,67,34)\n45 %in% c(2,5,8,23,67,34)\n\n# Check values from one vector present in another vector\nvec1 &lt;- c(2,5,8,23,67,34)\nvec2 &lt;- c(1,2,8,34) \nvec2 %in% vec1\n\n# Check values in a dataframe\ndf=data.frame(\n  emp_id=c(1,2,3,5),\n  name=c(\"John\",\"Rose\",\"Williams\", \"Ray\"),\n  dept_id=c(10,20,10,50)\n)\n\ndf$dept_state &lt;- if_else(df$dept_id %in% c(10,50),'NY','CA')\ndf\n\n\ndf2 &lt;- df[df$name %in% c('Ray','Rose'), ]\ndf2",
    "crumbs": [
      "Preparation"
    ]
  },
  {
    "objectID": "prepare.html#data-type-in-r",
    "href": "prepare.html#data-type-in-r",
    "title": "Preparation before workshop",
    "section": "Data type in R",
    "text": "Data type in R\n\n# numeric (double if with more than two decimal numbers)\nx &lt;- 10.5\nclass(x)\n\n# integer\nx &lt;- 1000L\nclass(x)\n\n\n# complex\nx &lt;- 9i + 3\nclass(x)\n\n# character/string\nx &lt;- \"R is exciting\"\nclass(x)\n\n# logical/boolean\nx &lt;- TRUE\nclass(x)\n\n# date\nx = \"01-06-2021\"\nx = as.Date(x, \"%d-%m-%Y\")\nclass(x)\n\n# Factors\n## Factors are the data objects which are used to categorize the data and store it as levels. \n## They can store both strings and integers. They are useful in the columns which have a limited number of unique values. Like Male/Female and True/False, etc. They are useful in data analysis for statistical modeling.\nx &lt;- c(\"East\",\"South\",\"East\",\"North\",\"North\",\"East\",\"West\",\"West\",\"West\",\"South\",\"North\")\nx_factor &lt;- factor(x) ### as.factor\nx_factor2 &lt;- factor(x, levels = c(\"East\", \"West\", \"South\", \"North\"))\nsummary(x_factor)\nsummary(x_factor2)\n\n# Missing values\nx &lt;- c(1, 2, NA, 4)\nis.na(x)\nwhich(is.na(x))\nx_omit &lt;- na.omit(x)\n\n\nNotes on NA\n\nA missing value in a factor variable is displayed as &lt;NA&gt; rather than just NA.\nR has a special value NaN for ‚Äúnot a number.‚Äù 0/0 is an example of a calculation that will produce a NaN. NaNs print as NaN, but generally act like NAs.\nAnother special case is Inf, such as log(0)\n\n\n\nExercise\n\nCreate a new R script\nInstall `tidyverse`` package and load it\nCreate a variable called first_num and assign it the value of 70802\nCreate a variable called first_char and assign it the value of my first character\nCreate a vector called gender, including: ‚Äúmale‚Äù, ‚Äúfemale‚Äù, ‚Äúother‚Äù, ‚Äúfemale‚Äù, ‚Äúmale‚Äù, ‚Äúfemale‚Äù, ‚Äúfemale‚Äù, ‚Äúother‚Äù, ‚Äúmale‚Äù. Make gender as a factor vector, following the order of ‚Äúfemale‚Äù, ‚Äúother‚Äù, and ‚Äúmale‚Äù.",
    "crumbs": [
      "Preparation"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction to R",
    "section": "",
    "text": "‚è∞ Time: 9:00 ‚Äì 11:00 AM, Friday, Feb 14th, 2025\nüìç Location: Journalism Building Room 140\nData wrangling/manipulation is the most important part when we deal with any types of data - before visualization and modeling. We will be mainly following R for Data Science, which includes importing data, transforming data, and tidying data.\nlibrary(tidyverse)\nlibrary(janitor)\nsetwd(\"~/Documents/Teaching/Workshop/beacon\") # good practice to set up your working directory",
    "crumbs": [
      "Introduction: Feb 14"
    ]
  },
  {
    "objectID": "slides/workshop1_slides.html#lazer-et-al.-2009",
    "href": "slides/workshop1_slides.html#lazer-et-al.-2009",
    "title": "BEACON Workshops Section 1",
    "section": "Lazer et al.¬†(2009):",
    "text": "Lazer et al.¬†(2009):\n\nOpportunities: individual, group, and societal level collected by new technologies and analyzed by new methods\nChallenges:\n\nnew paradigms needed for the massive dataset?\ninstitutional obstacle?\ndata access and privacy issue"
  },
  {
    "objectID": "slides/workshop1_slides.html#hilbert-et-al.-2019",
    "href": "slides/workshop1_slides.html#hilbert-et-al.-2019",
    "title": "BEACON Workshops Series 1",
    "section": "Hilbert et al.¬†(2019)",
    "text": "Hilbert et al.¬†(2019)\n\nParadigm change?\n\nObservational\nTheoretical research\nExperimental\n\nQuestions:\n\nMore data is good data?\nNew tools but new theories?\nEthical issues?\nResources"
  },
  {
    "objectID": "slides/workshop1_slides.html#aichner-et-al.-2021",
    "href": "slides/workshop1_slides.html#aichner-et-al.-2021",
    "title": "BEACON Workshops Series 1",
    "section": "Aichner et al.¬†(2021)",
    "text": "Aichner et al.¬†(2021)\n\nWhat is social media ‚Äì compared to mass media\n\nOnline platform: blogs, business networks, collaborative projects, enterprise social networks, forums, microblogs, photo sharing, products review, social bookmarking, social gaming, SN, video sharing, and virtual worlds.\n\nWhat do social media do?\n\nSocializing/Romance\nInteracting with government/politicians/companies/brands\nJob seeking and professional networking\nBusiness\n\nHow would you define it?"
  },
  {
    "objectID": "slides/workshop1_slides.html#road-map-for-next-two-weeks",
    "href": "slides/workshop1_slides.html#road-map-for-next-two-weeks",
    "title": "BEACON Workshops Series 1",
    "section": "Road map (for next two weeks)",
    "text": "Road map (for next two weeks)\n\nWhat is Computational Communication Science?\nWhat data is out there?\nHow to utilize the data?"
  },
  {
    "objectID": "slides/workshop1_slides.html#what-is-ccs",
    "href": "slides/workshop1_slides.html#what-is-ccs",
    "title": "BEACON Workshops Series 1",
    "section": "What is CCS?",
    "text": "What is CCS?\n\nCommunication as a field has put less emphasis on methodologies\n\n\n‚Äú‚Ä¶lack of methods for the study of [communication] process and adoption of approaches from other fields‚Äù1\n\n\nDefinition of Computational Communication Science (if we ever need one)?\n\n\nAn application of computational science to questions of human and social communication‚Äù2\n\nPoole, M. S. (2007). Generalization in process theories of communication. Communication Methods and Measures, 1(3), 181-190.Hilbert, M., Barnett, G., Blumenstock, J., Contractor, N., Diesner, J., Frey, S., ‚Ä¶ & Zhu, J. J. (2019). Computational communication science: A methodological catalyzer for a maturing discipline. International Journal of Communication, 13, 3912-3934."
  },
  {
    "objectID": "slides/workshop1_slides.html#what-is-ccs-1",
    "href": "slides/workshop1_slides.html#what-is-ccs-1",
    "title": "BEACON Workshops Series 1",
    "section": "What is CCS?",
    "text": "What is CCS?\n\nA subfield of Computational Social Science 1\n\nlarge and complex data sets;\nconsisting of digital traces and other ‚Äúnaturally occurring‚Äù data;\nrequiring algorithmic solutions to analyze;\nallowing the study of human communication by applying and testing communication theory.\n\n\nvan Atteveldt, W., & Peng, T. Q. (2018). When communication meets computation: Opportunities, challenges, and pitfalls in computational communication science. Communication Methods and Measures, 12(2-3), 81-92."
  },
  {
    "objectID": "slides/workshop1_slides.html#what-data-is-out-there-1",
    "href": "slides/workshop1_slides.html#what-data-is-out-there-1",
    "title": "BEACON Workshops Series 1",
    "section": "What data is out there?",
    "text": "What data is out there?\n\n\nDigital trace / Social media data\n\nAdministrative record\n\nTax records\nSchool records\nPhone call records"
  },
  {
    "objectID": "slides/workshop1_slides.html#what-kinds-of-social-media-data",
    "href": "slides/workshop1_slides.html#what-kinds-of-social-media-data",
    "title": "BEACON Workshops Series 1",
    "section": "What kinds of social media data?",
    "text": "What kinds of social media data?\n\nTwitter: text & network\nInstagram: image & text\nYoutube: image/video & text"
  },
  {
    "objectID": "slides/workshop1_slides.html#how-can-we-utilize-social-media-data",
    "href": "slides/workshop1_slides.html#how-can-we-utilize-social-media-data",
    "title": "BEACON Workshops Series 1",
    "section": "How can we utilize social media data?",
    "text": "How can we utilize social media data?\n\nSome applications of machine learning"
  },
  {
    "objectID": "slides/workshop1_slides.html#how-can-we-utilize-social-media-data-1",
    "href": "slides/workshop1_slides.html#how-can-we-utilize-social-media-data-1",
    "title": "BEACON Workshops Series 1",
    "section": "How can we utilize social media data?",
    "text": "How can we utilize social media data?\n\n\n\n\n\n\n\n\n\n\n\n(a) Unsupervised\n\n\n\n\n\n\n\n\n\n\n\n(b) Supervised\n\n\n\n\n\n\n\nFigure¬†1: An analogy of unsupervised and supervised machine learning"
  },
  {
    "objectID": "slides/workshop1_slides.html#how-can-we-utilize-social-media-data-2",
    "href": "slides/workshop1_slides.html#how-can-we-utilize-social-media-data-2",
    "title": "BEACON Workshops Series 1",
    "section": "How can we utilize social media data?",
    "text": "How can we utilize social media data?\n\nMeasurement\n\nAutomatic content analysis\n\nInference\n\nExplanatory modeling\nPredictive modeling"
  },
  {
    "objectID": "slides/workshop1_slides.html#characteristics-of-big-data",
    "href": "slides/workshop1_slides.html#characteristics-of-big-data",
    "title": "BEACON Workshops Series 1",
    "section": "Characteristics of big data",
    "text": "Characteristics of big data\n\n\nPRO\n\nBIGNESS\nALWAYS-ON\nNONREACTIVE\n\n\nCON\n\nINCOMPLETE\nINACCESSIBLE\nNONREPRESENTATIVE\nDRIFTING\nALGORITHMICALLY CONFOUNDED\nDIRTY AND NOISY\nSENSITIVE (potential harm?)"
  },
  {
    "objectID": "slides/workshop1_slides.html#is-computational-approach-a-one-for-all-solution",
    "href": "slides/workshop1_slides.html#is-computational-approach-a-one-for-all-solution",
    "title": "BEACON Workshops Series 1",
    "section": "IS COMPUTATIONAL APPROACH A ONE-FOR-ALL SOLUTION?",
    "text": "IS COMPUTATIONAL APPROACH A ONE-FOR-ALL SOLUTION?\nABSOLUTELY NOT!\n\nBig data hubris? ‚ÄúBig‚Äù data does not mean ‚ÄúGood‚Äù data\nValid and reliable measurement?\n\nValidation matters\n\nInterpretable machine learning?\n\nThe good-for-prediction vs.¬†The good-for-explanation\n\nEthics: the fairness of machine learning\n\nAlgorithmic bias ‚Äì annotation matters!\nCase: large language models, toxicity detection"
  },
  {
    "objectID": "slides/workshop1_slides.html#who-are-we",
    "href": "slides/workshop1_slides.html#who-are-we",
    "title": "BEACON Workshops Series 1",
    "section": "Who are we?",
    "text": "Who are we?\nThis series of computational workshops at the Manship School of Mass Communication is made possible through the collaborative efforts of:\n\nBEACON Lab led by Drs. Ryan Wang & Josh Jackson (Behavioral and Empirical Approaches for COmputational Nexus): The BEACON Lab primarily focuses on advancing (computational) methodologies as well as introducing analytical skills.\nDiCCAL Research Group led by Drs. Janice Lee & Royce Choi (Digital Cognition and Computational Analytics Lab).The DiCCAL Research Group emphasizes applying these techniques within the fields of communication and social science.\n\nFeb 14: Data wrangling\nFeb 28: Data visualization\nMar 21: Basic textual analysis"
  },
  {
    "objectID": "slides/workshop1_slides.html#why-r",
    "href": "slides/workshop1_slides.html#why-r",
    "title": "BEACON Workshops Series 1",
    "section": "Why R?",
    "text": "Why R?\nWe will be mainly using R(programming language) and RStudio (Integrated development environment, IDE)}.\n\nOpen-source with a wide variety of statistics-related libraries\nLarge online community\nGreat visualization: C√©dric Scherer, BBC\nYou can also make website, slides, and etc. beyond coding\nPopular programming language in tech companies and academia\n\nWhat about Python?"
  },
  {
    "objectID": "slides/workshop1_slides.html#some-notes",
    "href": "slides/workshop1_slides.html#some-notes",
    "title": "BEACON Workshops Series 1",
    "section": "Some notes",
    "text": "Some notes\n\nNot trying to pack everything about R in three workshops. But hope to spark your interest in R.\nThere will be a learning curve when transitioning from a graphical user interface (GUI)-oriented software to a syntax-based programming language\nConsistent practice is key to becoming proficient in a new language.\nWe always here to help! And peer learning is also another great way to gain insights. No dumb questions!\nWickham, H., C Ãß etinkaya-Rundel, M., & Grolemund, G. (2023). R for data science.\nR-bloggers\nStack Overflow\n(GP)TA: ChatGPT, Claude.ai, Deepseek (In my experience, ChatGPT performs pretty well in providing answers or guidance for any questions related to R language, particularly when employing proper prompts.\nBeyond these workshops: SICSS"
  },
  {
    "objectID": "intro.html#understanding-data-frame",
    "href": "intro.html#understanding-data-frame",
    "title": "Introduction to R",
    "section": "Understanding data frame",
    "text": "Understanding data frame\n\nCreate a data frame\n\nheight &lt;- c(180, 155, 160, 167, 181)\nweight &lt;- c(65, 50, 52, 58, 70)\nnames &lt;- c(\"Joanna\", \"Charlotte\", \"Helen\", \"Karen\", \"Amy\")\n\ndata &lt;- data.frame(height = height, weight = weight, names = names) #stringsAsFactors = TRUE\ndata\ndim(data)\nstr(data)\n\n\n\nPositional index\n\ndata[1,3] # the first value (1st row ) of the names variable (3th column)\ndata$names[1]\ndata[1:2, 2:3] #  the first 2 rows and the last 2 columns\ndata[1:2, ]\ndata[, 2:3]\n\n\n\nOrdering data frames\n\nheight_order &lt;- data[order(data$height), ]\nrownames(height_order) &lt;- 1:nrow(height_order)\nheight_order\n\nheight_order &lt;- data[order(data$height, decreasing = T),]\n\n\n\nAdding/Removing columns and rows\n\ndata2 &lt;- data.frame(state = c(\"NY\", \"PA\", \"MD\", \"VA\", \"MA\"))\ndata_newcolumn &lt;- cbind(data, data2)\n\ndata_removecolumn &lt;- data_newcolumn[, c(1:2, 4)]\ndata_newcolumn$state &lt;- NULL\n\ndata3 &lt;- data.frame(height = c(120, 150, 132, 122),\n                    weight = c(44, 56, 49, 45),\n                    names = c(\"Ryan\", \"Chris\", \"Ben\", \"John\"))\ndata_newrow &lt;- rbind(data, data3)\ndata_removerow &lt;- data_newrow[c(1,6:9),]\n\n\n\nMerging data frames\nHere are two fictitious datasets of a clinical trial. One table contains demographic information of the patients and the other one adverse events recorded throughout the course of the trial.\n\ndemographics &lt;- data.frame(\n  id = c(\"P1\", \"P2\", \"P3\"),\n  age = c(40, 54, 47),\n  state = c(\"NY\", \"MA\", \"PA\"),\n  stringsAsFactors = FALSE\n)\n\nadverse_events &lt;- data.frame(\n  id = c(\"P1\", \"P1\", \"P3\", \"P4\"),\n  term = c(\"Headache\", \"Neutropenia\", \"Constipation\", \"Tachycardia\"),\n  onset_date = c(\"2020-12-03\", \"2021-01-03\", \"2020-11-29\", \"2021-01-27\"),\n  stringsAsFactors = FALSE\n)\n\n\nmerge(demographics, adverse_events, by = \"id\")\nmerge(demographics, adverse_events, by = \"id\", all.x = T)\nmerge(demographics, adverse_events, by = \"id\", all.y = T)\nmerge(demographics, adverse_events, by = \"id\", all = T)\n\n\nadverse_events2 &lt;- adverse_events\ncolnames(adverse_events2)[1] &lt;- \"pat_id\"\nmerge(demographics, adverse_events2, by.x = \"id\", by.y = \"pat_id\", all = T)\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nPlease merge the following two datasets emp_df (employee information)and dept_df (department information) using two ID columns dept_id and dept_branch_id.\n\nemp_df=data.frame(\n  emp_id=c(1,2,3,4,5,6),\n  name=c(\"Chris\",\"Rose\",\"Williams\",\"Jones\",\"Jayden\",\"Xavior\"),\n  superior_emp_id=c(1,1,1,2,2,2),\n  dept_id=c(10,20,10,10,40,30),\n  dept_branch_id= c(101,102,101,101,104,103)\n)\n\ndept_df=data.frame(\n  dept_id=c(10,20,30,40),\n  dept_name=c(\"Finance\",\"Marketing\",\"Sales\",\"IT\"),\n  dept_branch_id= c(101,102,103,104)\n)\n\n\n\nShow the code\nmerge(emp_df, dept_df, by = c(\"dept_id\", \"dept_branch_id\"), all.x = T)",
    "crumbs": [
      "Introduction: Feb 14"
    ]
  },
  {
    "objectID": "intro.html#data-importing-and-exporting",
    "href": "intro.html#data-importing-and-exporting",
    "title": "Introduction to R",
    "section": "Data importing (and exporting)",
    "text": "Data importing (and exporting)\nCommon data formats:\n\nCSV (comma-seperated values) / TSV (tab-seperated values)\nxlsx\ntxt\nother softwares/packages: .sav(SPSS), .dta(STATA)\n\nCommon data types in R:\n\ndata frame\ntibble (tbl_df): it does much less than a data frame (a neater data frame), as it never changes the type of the inputs (e.g.¬†it keeps list columns unchanged, and never converts strings to factors), it never changes the names of variables, it only recycles inputs of length 1, and it never creates row.names().\n\n\ndata &lt;- data.frame(a = 1:26, b = letters[1:26], c = Sys.Date() - 1:26)\ndata\nas_tibble(data)\n\n\nstudents &lt;- read.csv(\"https://pos.it/r4ds-students-csv\") # from URL\nstudents &lt;- read.csv(\"data/students.csv\") # from local\nstudents\nstr(students)\nsummary(students)\n\n\nstudents &lt;- read.csv(\"data/students.csv\", na.strings=c(\"N/A\", \"\"))\n# students &lt;- read_csv(\"data/students.csv\", na = c(\"N/A\", \"\"))\n\n\nBasic data cleaning\n\nstr(students)\nstudents |&gt;\n  rename(student_id = Student.ID,\n         full_name = Full.Name,\n         fav_food = favourite.food)\n\nrename(students,\n       student_id = Student.ID,\n       full_name = Full.Name,\n       fav_food = favourite.food)\n\nstudents_rename &lt;- students |&gt;\n                   rename(student_id = Student.ID,\n                          full_name = Full.Name,\n                          fav_food = favourite.food)\n\n\n# a faster way\nstudents_rename &lt;- clean_names(students)\nstudents_rename &lt;- mutate(students_rename, meal_plan = factor(meal_plan))\nstr(students_rename)\n\n\nstudents_clean &lt;- students_rename |&gt;\n                  mutate(age = if_else(age == \"five\", \"5\", age))\n\n#students_rename2 &lt;- students_rename\n#students_rename2$age &lt;- ifelse(students_rename2$age == \"five\", 5, students_rename2$age)\n#students_rename2$age[students_rename2$age == \"five\"] &lt;- 5\n\nNote: if_else() has three arguments. The first argument test should be a logical vector. The result will contain the value of the second argument, yes, when test is TRUE, and the value of the third argument, no, when it is FALSE. Here we‚Äôre saying if age is the character string ‚Äúfive‚Äù, make it ‚Äú5‚Äù, and if not leave it as age.\n\nstudents_clean_final &lt;- students_clean |&gt;\n                        mutate(group = case_when(\n                          age &lt;= 5 ~ \"young\",\n                          age &gt; 5 ~ \"old\",\n                          .default = \"other\"\n                        ))\n\n\n\nShow the code\nstudents_clean_final &lt;- students |&gt;\n                        clean_names() |&gt;\n                        mutate(meal_plan = factor(meal_plan),\n                               age = parse_number(if_else(age == \"five\", \"5\", age)),\n                               group = case_when(\n                                       age &lt;= 5 ~ \"young\",\n                                       age &gt; 5 ~ \"old\",\n                                       .default = \"other\"))\n\n\n\nwrite.csv(students_clean_final, \"data/students_final.csv\", row.names = F)",
    "crumbs": [
      "Introduction: Feb 14"
    ]
  },
  {
    "objectID": "intro.html#data-tidying",
    "href": "intro.html#data-tidying",
    "title": "Introduction to R",
    "section": "Data tidying",
    "text": "Data tidying\nIn real life, the data you collected is ‚Äúmessy‚Äù and ‚Äúdirty‚Äù.\n\nData Scientists spend up to 80% of the time on data cleaning and 20 percent of their time on actual data analysis. 1\n\nThe process of ‚Äútidying‚Äù data would thus create what‚Äôs known as tidy data, as populated by Hadley Wickham (one of the authors of R for Data Science).\n\nTidy datasets provide a standardized way to link the structure of a dataset (its physical layout) with its semantics (its meaning). 2\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\n\n\n\n‚ùì Question: are they the same datasets? Which one is easier to work with and why?\n\n\n\n\n\n\n\ntable1\n\n# A tibble: 6 √ó 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\ntable2\n\n# A tibble: 12 √ó 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable3\n\n# A tibble: 6 √ó 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\nA tidy data set is:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\nSource: r4ds\n\n\nLet‚Äôs work with the table1 here.\n\n# Compute rate per 10,000\ntb_table &lt;- table1 |&gt;\n  mutate(rate = cases / population * 10000)\n\n\ntb_year &lt;- table1 |&gt; \n  group_by(year) |&gt; \n  summarize(total_cases = sum(cases))\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nUsing table1 to calculate the TB cases per year in each country. (Hint: use mean())\n\n\nShow the code\ntable1 |&gt; \n  group_by(country) |&gt; \n  summarize(mean_cases = mean(cases))",
    "crumbs": [
      "Introduction: Feb 14"
    ]
  },
  {
    "objectID": "intro.html#data-transformation",
    "href": "intro.html#data-transformation",
    "title": "Introduction to R",
    "section": "Data transformation",
    "text": "Data transformation\n\nColumns and rows\nflights is a dataset on flights that departed from New York City in 2013.\n\n#install.packages(\"nycflights13\")\nlibrary(nycflights13)\nflights\nstr(flights)\n\n\n# Subseting certain columns\nflights_sub &lt;- flights |&gt;\n  select(c(month, day, flight, carrier, origin, dest, distance, air_time))\n\n# Creating new columns that are derived from the existing columns\nflights_sub &lt;- flights_sub |&gt; \n  mutate(speed = distance / air_time * 60)\n\n\n# Filtering certain rows\nflights_IAH &lt;- flights |&gt;\n  filter(dest == \"IAH\")\n\nflights_summer &lt;- flights |&gt;\n   filter(month == 6 | month == 7 | month == 8) #OR\n\nflights_summer &lt;- flights |&gt;\n   filter(month %in% c(6,7,8))\n\nflights_jan1 &lt;- flights |&gt; \n  filter(month == 1 & day == 1) #AND\n\n\nflights |&gt; \n  arrange(year, month, day, dep_time)\n\nflights |&gt; \n  arrange(desc(dep_delay))\n\n\nflights |&gt; \n  distinct(origin, dest, .keep_all = TRUE)\n\n\nflights |&gt; \n  group_by(month) |&gt; \n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n()\n  )\n\ndaily_flights &lt;- flights |&gt;\n  group_by(year, month, day) |&gt;\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\nUsing the same flights dataset to creat a new dataframe flights_houston, and:\n\n\nOnly include flights heading to Houston (IAH and HOU)\nCalcuate the speed (\\(speed = distance / air\\_time * 60\\))\nOnly keep the columns of ‚Äúyear‚Äù, ‚Äúmonth‚Äù, ‚Äúday‚Äù,‚Äúdep_time‚Äù, ‚Äúcarrier‚Äù, ‚Äúflight‚Äù, and ‚Äúspeed‚Äù\nArrange the data based on the speed with a desceding order.\n\n\n\nShow the code\nflights_houston &lt;- flights |&gt; \n  filter(dest == \"IAH\" | dest == \"HOU\") |&gt; \n  mutate(speed = distance / air_time * 60) |&gt; \n  select(year:day, dep_time, carrier, flight, speed) |&gt; \n  arrange(desc(speed))\n\n\n\nUsing the same flights dataset to find out which carrier heading to which airport has the worst average delays?\n\n\n\nShow the code\ndelay_flights &lt;- flights |&gt;\n  group_by(carrier, dest) |&gt;\n  summarize(\n    mean_delay = mean(dep_delay, na.rm = TRUE), \n    n = n())\n\n\n\n\n\n\n\nLengthening and widening data\nIn reality, you need long-format data much more commonly than wide-format data (such as visualizing in ggplot2 and modeling).\n\nWide format data: it has a column for each variable and a long format data. The billboard dataset records the billboard rank of songs in the year 2000:\n\n\nbillboard\n\n\nLong format data: it has a column for possible variable types and a column for the values of those variables. cms_patient_experience, is a dataset from the Centers of Medicare and Medicaid services that collects data about patient experiences:\n\n\ncms_patient_experience\n\ntidyr provides two functions for pivoting data:\n\npivot_longer(): it takes wide-format data and turn it into long-format data (melt in reshape2). \npivot_wider(): it takes long-format data and turn it into wide-format data (cast in reshape2).\n\n\nbillboard |&gt; \n  pivot_longer(\n    cols = starts_with(\"wk\"), \n    names_to = \"week\", \n    values_to = \"rank\"\n  )\n\n\nbilloard_longer &lt;- billboard |&gt; \n                    pivot_longer(\n                      cols = starts_with(\"wk\"), \n                      names_to = \"week\", \n                      values_to = \"rank\",\n                      values_drop_na = TRUE) |&gt; \n                    mutate(week = parse_number(week))\n\nBut in reality, we might need to deal with multiple variables at the same time‚Ä¶ Now, let‚Äôs take a look at the who2 dataset, the source of table1 that you saw above.\n\nwho2\n\nThis dataset is collected by the World Health Organisation, records information about tuberculosis diagnoses. There are two columns that are already variables and are easy to interpret: country and year. They are followed by 56 columns like ‚Äúsp_m_014‚Äù, ‚Äúep_m_4554‚Äù, and ‚Äúrel_m_3544‚Äù. Each column name is made up of three pieces separated by ‚Äú_‚Äù. The first piece,‚Äúsp/rel/ep‚Äù, describes the method used for the diagnosis, the second piece, ‚Äúm/f‚Äù is the gender (coded as a binary variable in this dataset), and the third piece, ‚Äú014/1524/2534/3544/4554/5564/65‚Äù is the age range (‚Äú014‚Äù represents ‚Äú0-14‚Äù, for example).\n\nwho2_long &lt;- who2 |&gt; \n              pivot_longer(\n                cols = !(country:year),\n                names_to = c(\"diagnosis\", \"gender\", \"age\"), \n                names_sep = \"_\",\n                values_to = \"count\"\n              )\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nThe following (wide) dataset shows the number of points scored by various NBA basketball players in different years. Please tranform it to a long format.\n\nnba &lt;- data.frame(player=c('Joel Embiid', 'Luka Doncic', 'Jayson Tatum', 'Trae Young'),\n                 year1=c(28.5, 27.7, 26.4,25.3),\n                 year2=c(30.6, 28.4, 26.9, 28.4),\n                 year3=c(33.1, 32.4, 30.1, 26.2))\n\n\n\nShow the code\nnba_long &lt;- nba |&gt;\n  pivot_longer(cols=c('year1', 'year2', 'year3'),\n               names_to='year',\n               values_to='points')\nnba_long",
    "crumbs": [
      "Introduction: Feb 14"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "Introduction to R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDasu, T. & Johnson, T. (2003). Exploratory Data Mining and Data Cleaning.‚Ü©Ô∏é\nWickham, H. (2014). Journal of Statistical Software, 59(10), 1‚Äì23. https://doi.org/10.18637/jss.v059.i10‚Ü©Ô∏é",
    "crumbs": [
      "Introduction: Feb 14"
    ]
  },
  {
    "objectID": "intro.html#simple-statistics",
    "href": "intro.html#simple-statistics",
    "title": "Introduction to R",
    "section": "Simple statistics",
    "text": "Simple statistics\nLet‚Äôs play with a built-in dataset iris. This data set provides the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. (see ?iris for more detail).\n\nT-Test and ANOVA\n\ndata(iris)\nstr(iris)\nsummary(iris)\n\nIf we wanted to test whether the mean height of sepal in this sample is equal to 6 cm (mu = 6), assuming these data are normally distributed, we can use a t.test() to do so.\n\nt.test(iris$Sepal.Length, mu = 6)\n\n\nres_aov &lt;- aov(Sepal.Length ~ Species, data = iris)\nsummary(res_aov)\nboxplot(Sepal.Length ~ Species, data = iris)\n\n\n\nCorrelation\n\ncor(iris$Sepal.Length, iris$Petal.Length)\ncor.test(iris$Sepal.Length, iris$Petal.Length)\n\n\niris_mat &lt;- iris[, -5]\ncor(iris_mat)\n\n\n\nSimple regression\n\nreg1 &lt;- lm(Sepal.Length ~ Species, data = iris)\nsummary(reg1)\n\n\nreg2 &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width , data = iris)\nsummary(reg2)\n\n\nlibrary(lme4)\nreg3 &lt;- lmer(Sepal.Length ~ Sepal.Width + Petal.Length + Petal.Width + (1 | Species) , data = iris)\nsummary(reg3)\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\nFind your favorite datasets data() or import your own data. Perform the basic statistic analysis as shown above (based on the research questions you are interested in).",
    "crumbs": [
      "Introduction: Feb 14"
    ]
  }
]